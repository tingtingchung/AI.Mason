**Class 6: RNN**  
[M3-1 Featurizing Documents Digitally](https://www.dropbox.com/scl/fi/5syj8l2q1gl30tuxy5qce/M3-1-featurizing-documents-digitally.pptx?rlkey=nnl9c7yzx9jujhd89rw9xnmcr&dl=0)  
[Colab: Tokenization](https://colab.research.google.com/drive/16yYaORYT9hU6OC3xZQGzmyuGm9MUHgDq)  
[M3-2 Embeddings](https://www.dropbox.com/scl/fi/aa3vuov9o105xsrvf4fs6/M3-2-embeddings.pptx?rlkey=0w897if9s4f6fgso1ondx2n93&dl=0)  
[Tensorflow's Embedding Explorer](https://projector.tensorflow.org/)  
[M3-3 RNN](https://www.dropbox.com/scl/fi/n29yaptji7s809al1fyq5/M3-3-rnn.pptx?rlkey=cg6he9wjzm0yes64d0lacqqsd&dl=0)  
[M3-4 RNN for Document Classification](https://www.dropbox.com/scl/fi/lhubtpz6khf6fgifrpohs/M3-4-rnn-for-document-classification.pptx?rlkey=07kbmgxhs2q1vnolygsxsfwkr&dl=0)  
[Dr. Chung's TF Keras RNN Model Template](https://docs.google.com/document/d/1uPGD60oGgf40YOzyid0SfaTYNBtkSRlGvhud8ALB6tA/edit?usp=sharing)  

**Complete these readings/videos before next class**  
[Keras LSTM Layer](https://keras.io/api/layers/recurrent_layers/lstm/)  
[How AI Works by Nir Zicherman](https://every.to/p/how-ai-works?fbclid=IwAR2KWfiKq627x9SxpTpZojaxHSjaA0zcEELySUyEGhD7jbWzcS3vFNyJ4OI)  
[A Word is Worth A Thousand Vectors by Chris Moody at Stitch Fix](https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/)  

*Optional References*  
[word2vec](https://code.google.com/archive/p/word2vec/)  
[GloVe](https://nlp.stanford.edu/projects/glove/)  
[Colab: Bag-of-N-Grams](https://colab.research.google.com/github/practical-nlp/practical-nlp/blob/master/Ch3/03_Bag_of_N_Grams.ipynb?authuser=0&pli=1)  
[BERT Preprocessing](https://www.tensorflow.org/text/guide/bert_preprocessing_guide)  
[Classifying text with BERT](https://www.tensorflow.org/text/tutorials/classify_text_with_bert)  

*Optional Fun Stuff*  
[Natural Language Processing by CrashCourse AI](https://www.pbs.org/video/natural-language-processing-7-eroyod/)  

**Class 7: LSTM & LLM**  
[M3-5 LSTM](https://www.dropbox.com/scl/fi/oqfg9tax1ydr11oxxl1gp/M3-5-LSTM.pptx?rlkey=g9sqqbaiil7h0m3fdbltsef35&dl=0)  
[Colab: RNN + LSTM for IMDB sentiment analysis](https://colab.research.google.com/drive/1YSOMgbXHJrOnGS7Vjtob9-etAxHjF_0_?usp=sharing)  

**Complete these readings/videos before next class**  
[ISLP:](https://hastie.su.domains/ISLP/ISLP_website.pdf.download.html)  
> Chapter 10.6: When to Use Deep Learning  
> Chapter 10.7.2: Regularization and Stochastic Gradient Descent (focusing on regularization onlyâ€”we read SGD previously)  
> Chapter 10.7.3: Dropout Learning  
> Chapter 10.7.4: Network Tuning  
> Chapter 10.8: Interpolation and Double Descent

[Godbole, V., Dahl, G. E., Gilmer, J., Shallue, C. J., & Nado, Z. (n.d.) Deep Learning Tuning Playbook](https://github.com/google-research/tuning_playbook)  
[Keras Tuner](https://www.tensorflow.org/tutorials/keras/keras_tuner)  
Wilber, J., & Werness, B. (2021, January). [The bias variance tradeoff](https://mlu-explain.github.io/bias-variance/). MLU-Explain.  
Wilber, J., & Werness, B. (2021, December). [Double descent part 1: A visual introduction](https://mlu-explain.github.io/double-descent/). MLU-Explain.  
Werness, B., & Wilber, J. (2021, December). [Double descent part 2: A mathematical explanation](https://mlu-explain.github.io/double-descent2/). MLU-Explain.

*Optional References*  
In May 2023, a William & Mary MSBA student team won first place in the CSBS Data Analytics Competition with an LSTM model under Dr. Chung's supervision! Their competition paper is available to read online [here](https://www.csbs.org/data-analytics-competition)  
