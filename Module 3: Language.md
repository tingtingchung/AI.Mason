**Class 5: Vision**  
[M3-4 CNN3A: Convolution](https://www.dropbox.com/scl/fi/n7t8fi27nu3tadcn0i0wj/M3-4-CNN3A-conv.pptx?rlkey=eazdb831tp0w4umi40sl28rku&dl=0)  
[M3-5 CNN3B: Pooling](https://www.dropbox.com/scl/fi/xnj81rtqx1bp0r0ie3eup/M3-5-CNN3B-pooling.pptx?rlkey=dyb24vnsgtqxs54p6r5b36y5b&dl=0)  
[M3-6 CNN3C: Putting Everything Together](https://www.dropbox.com/scl/fi/85qk7ooo1fp4vybhpmknc/M3-6-CNN3C-putting-everything-together.pptx?rlkey=ga0uyp2ib4x4ty9j4guiq8voo&dl=0)   
[Andrew Ng on how AI can power any business](https://www.ted.com/talks/andrew_ng_how_ai_could_empower_any_business?language=en)  

[Colab: MNIST with CNN](https://colab.research.google.com/drive/15Udjs_HjIopW0R18f9lzznXnk63tzeX8) 

**Complete these readings/videos before next class**  
[Chapter 10.4 & 10.5 Recurrent Neural Networks, ISLP](https://hastie.su.domains/ISLP/ISLP_website.pdf.download.html)  
[TF Word Embeddings](https://www.tensorflow.org/text/guide/word_embeddings)  
[TF Tokenizers](https://www.tensorflow.org/text/guide/tokenizers)  
[Keras Recurrent Layers](https://keras.io/api/layers/recurrent_layers/)  
[Keras Preprocessing Layers](https://keras.io/api/layers/preprocessing_layers/)  
[Keras TextVectorization Layer](https://keras.io/api/layers/preprocessing_layers/text/text_vectorization/)  
[Sparse Matrix Format in Tensorflow](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor)  
[Masking & Padding in Keras](https://www.tensorflow.org/guide/keras/masking_and_padding#:~:text=Padding%20is%20a%20special%20form,pad%20or%20truncate%20some%20sequences.)  

*Optional References*  
[Karpathy, A., Johnson, J., & Fei-Fei, L. (2015). Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078.](http://vision.stanford.edu/pdf/KarpathyICLR2016.pdf)   
[A Critical Review of Recurrent Neural Networks for Sequence Learning by Zack Lipton et al. 2015](https://arxiv.org/abs/1506.00019)  
[Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.](https://arxiv.org/abs/1706.03762)

*Optional Fun Stuff*  
[Real life example of matrix |An image is nothing but a matrix of pixel values.](https://www.youtube.com/watch?v=RDBWWZ7o5sQ)  
[Cats vs. Dogs? Lets Make an AI to Settle This by CrashCourse AI](https://www.pbs.org/video/cats-vs-dogs-lets-make-an-ai-to-settle-this-lab-19-rp1lwa/) 

**Class 6: RNN for Language**  
[M3-1 Featurizing Documents Digitally](https://www.dropbox.com/scl/fi/5syj8l2q1gl30tuxy5qce/M3-1-featurizing-documents-digitally.pptx?rlkey=nnl9c7yzx9jujhd89rw9xnmcr&dl=0)   
[Tiktokenizer](https://tiktokenizer.vercel.app/): Online playground for openai/tiktoken, calculating the correct number of tokens for a given prompt  
[Worksheet #07: Language - Featurizing Documents Digitally & Embeddings](https://docs.google.com/document/d/1V8E4GbSREKjZ0sLr1rhJjqCmWikHbAJRu5GZ2ziAwFw/edit?tab=t.0) & [Solution](https://docs.google.com/document/d/1OCrbBKYL5aGcPTz-qS-KgJeLU4fznyWvFbv4htu9VIQ/edit?usp=sharing)    
[Colab: Tokenization](https://colab.research.google.com/drive/16yYaORYT9hU6OC3xZQGzmyuGm9MUHgDq)  
[Small Language Model Demo](https://www.cs.cmu.edu/~pvirtue/AIS/ngrams/ngrams.html)  
[M3-2 Embeddings](https://www.dropbox.com/scl/fi/aa3vuov9o105xsrvf4fs6/M3-2-embeddings.pptx?rlkey=0w897if9s4f6fgso1ondx2n93&dl=0)    
[Tensorflow's Embedding Explorer](https://projector.tensorflow.org/)  
[M3-3 RNN](https://www.dropbox.com/scl/fi/n29yaptji7s809al1fyq5/M3-3-rnn.pptx?rlkey=cg6he9wjzm0yes64d0lacqqsd&dl=0)  
[Excel Worksheet: RNN](https://www.dropbox.com/scl/fi/g7t9nhunmlxj1x9roubo3/m4-language-worksheets.xlsx?rlkey=pocyqdsmu7tk7bizdav00gqhd&dl=0)  
[M3-4 RNN for Document Classification](https://www.dropbox.com/scl/fi/lhubtpz6khf6fgifrpohs/M3-4-rnn-for-document-classification.pptx?rlkey=07kbmgxhs2q1vnolygsxsfwkr&dl=0)  

**Complete these readings/videos before next class**  
[Keras LSTM Layer](https://keras.io/api/layers/recurrent_layers/lstm/)  
[How AI Works by Nir Zicherman](https://every.to/p/how-ai-works?fbclid=IwAR2KWfiKq627x9SxpTpZojaxHSjaA0zcEELySUyEGhD7jbWzcS3vFNyJ4OI)  
[A Word is Worth A Thousand Vectors by Chris Moody at Stitch Fix](https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/)  

*Optional References*  
[word2vec](https://code.google.com/archive/p/word2vec/)  
[GloVe](https://nlp.stanford.edu/projects/glove/)  
[Colab: Bag-of-N-Grams](https://colab.research.google.com/github/practical-nlp/practical-nlp/blob/master/Ch3/03_Bag_of_N_Grams.ipynb?authuser=0&pli=1)  
[BERT Preprocessing](https://www.tensorflow.org/text/guide/bert_preprocessing_guide)  
[Classifying text with BERT](https://www.tensorflow.org/text/tutorials/classify_text_with_bert)  

*Optional Fun Stuff*  
[Natural Language Processing by CrashCourse AI](https://www.pbs.org/video/natural-language-processing-7-eroyod/)  

**Class 7: LSTM & LLM**  
[M3-5 LSTM](https://www.dropbox.com/scl/fi/oqfg9tax1ydr11oxxl1gp/M3-5-LSTM.pptx?rlkey=g9sqqbaiil7h0m3fdbltsef35&dl=0)  
[Excel Worksheet: LSTM](https://www.dropbox.com/scl/fi/wey1urooi3rbqgs0wxj39/m4-LSTM-in-Excel.xlsx?rlkey=rhxnb6qfrmbe0w4lmpe1g21uz&dl=0)  
[Colab: RNN + LSTM for IMDB sentiment analysis](https://colab.research.google.com/drive/1YSOMgbXHJrOnGS7Vjtob9-etAxHjF_0_?usp=sharing)  
[Excel Worksheet: Transformer](https://www.dropbox.com/scl/fi/4w458wv2l9eigsnno1w1f/m4-Transformer-in-Excel.xlsx?rlkey=kl6q25b93kvok6ckz29nrib16&dl=0)  

**Complete these readings/videos before next class**  
[ISLP:](https://hastie.su.domains/ISLP/ISLP_website.pdf.download.html)  
> Chapter 10.6: When to Use Deep Learning  
> Chapter 10.7.2: Regularization and Stochastic Gradient Descent (focusing on regularization onlyâ€”we read SGD previously)  
> Chapter 10.7.3: Dropout Learning  
> Chapter 10.7.4: Network Tuning  
> Chapter 10.8: Interpolation and Double Descent

[Godbole, V., Dahl, G. E., Gilmer, J., Shallue, C. J., & Nado, Z. (n.d.) Deep Learning Tuning Playbook](https://github.com/google-research/tuning_playbook)  
[Keras Tuner](https://www.tensorflow.org/tutorials/keras/keras_tuner)  
Wilber, J., & Werness, B. (2021, January). [The bias variance tradeoff](https://mlu-explain.github.io/bias-variance/). MLU-Explain.  
Wilber, J., & Werness, B. (2021, December). [Double descent part 1: A visual introduction](https://mlu-explain.github.io/double-descent/). MLU-Explain.  
Werness, B., & Wilber, J. (2021, December). [Double descent part 2: A mathematical explanation](https://mlu-explain.github.io/double-descent2/). MLU-Explain.

*Optional References*  
In May 2023, a William & Mary MSBA student team won first place in the CSBS Data Analytics Competition with an LSTM model under Dr. Chung's supervision! Their competition paper is available to read online [here](https://www.csbs.org/data-analytics-competition)  
